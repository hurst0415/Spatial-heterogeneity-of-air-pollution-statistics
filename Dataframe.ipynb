{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.integrate import odeint\n",
    "import statistics\n",
    "from scipy.stats import lognorm\n",
    "import math\n",
    "import string\n",
    "import sdeint\n",
    "from joblib import Parallel, delayed \n",
    "import multiprocessing\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import optimize\n",
    "from scipy.stats import expon\n",
    "import matplotlib.ticker as mtick\n",
    "from scipy.stats import rv_continuous\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sympy import *\n",
    "from sympy import summation, symbols, solve, Function, Sum, log\n",
    "from numpy import random\n",
    "from mpl_toolkits.axes_grid.inset_locator import (inset_axes, InsetPosition,mark_inset)\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for the q-exponential function\n",
    "def q_exp_func(x,q,l):\n",
    "    #q = q_value\n",
    "    return (2 - q) * l * np.sign(1 + (q- 1) * l * x)* (np.abs(1 + (q- 1) * l * x))**( 1/(1 - q))\n",
    "\n",
    "#function for the exponential function\n",
    "def exp_func(x,l):\n",
    "    return l*np.exp(-l*x)   \n",
    "\n",
    "#class for generating instances of the q-exponential function\n",
    "class qExp_gen(rv_continuous): \n",
    "    \"q exponential\"\n",
    "    #pdf defines the pdf\n",
    "    def _pdf(self, x, l , q):\n",
    "        self.l=l;\n",
    "        self.q=q  \n",
    "        if q==1: \n",
    "            return exp_func(x,l)\n",
    "        else:\n",
    "            return q_exp_func(x,q,l)\n",
    "\n",
    "    def _stats(self,l, q):\n",
    "        return [self.l,self.q,0,0]\n",
    "    #fitstart provides a starting point for any MLE fit\n",
    "    def _fitstart(self,data):\n",
    "        return (1.1,1.1)\n",
    "    #argcheck tests that the parameters are meaningful\n",
    "    def _argcheck(self, l, q):\n",
    "        return (l>0)&(q>0)\n",
    "    \n",
    "# check if a string is NAN   \n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "\n",
    "#read sites meta\n",
    "europe_meta=pd.read_csv('data_sites.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ) Define a function which match site area and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter site code\n",
    "def type_area_sort(x):\n",
    "    site_area = df[df[\"code\"]==x]['area']\n",
    "    site_type = df[df[\"code\"]==x]['type']\n",
    "    if site_area.to_string(index = False) in [\" rural\",\" rural_remote\",\" rural_regional\",\" rural_nearcity\"]:\n",
    "        if site_type.to_string(index = False) in [\" background\"]:\n",
    "            return \"rural background\"\n",
    "        elif site_type.to_string(index = False) in [\" traffic\"]:\n",
    "            return \"suburban/rural traffic\"\n",
    "        elif site_type.to_string(index = False) in [\" industrial\"]:\n",
    "            return \"suburban/rural industrial\"\n",
    "        else:\n",
    "            return np.nan\n",
    "    elif site_area.to_string(index = False) in [\" urban\"]:        \n",
    "        if site_type.to_string(index = False) in [\" background\"]:\n",
    "            return \"urban background\"\n",
    "        elif site_type.to_string(index = False) in [\" traffic\"]:\n",
    "            return \"urban traffic\"\n",
    "        elif site_type.to_string(index = False) in [\" industrial\"]:\n",
    "            return \"urban industrial\"\n",
    "        else:\n",
    "            return np.nan\n",
    "    elif site_area.to_string(index = False) in [\" suburban\"]:        \n",
    "        if site_type.to_string(index = False) in [\" background\"]:\n",
    "            return \"suburban background\"\n",
    "        elif site_type.to_string(index = False) in [\" traffic\"]:\n",
    "            return \"suburban/rural traffic\"\n",
    "        elif site_type.to_string(index = False) in [\" industrial\"]:\n",
    "            return \"suburban/rural industrial\"\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ) Define a function for computing mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_output(code,pol):\n",
    "    #check if the location has name\n",
    "    if isNaN(europe_meta[europe_meta['site']==code]['site_name']).bool():\n",
    "        return (np.nan,np.nan)\n",
    "    #check if the location can be found on map\n",
    "    elif math.isnan(europe_meta[europe_meta['site']==code]['latitude']):\n",
    "        return (np.nan,np.nan)\n",
    "    elif math.isnan(europe_meta[europe_meta['site']==code]['longitude']):\n",
    "        return (np.nan,np.nan)\n",
    "    #check if the area type is defined\n",
    "    elif isNaN(europe_meta[europe_meta['site']==code]['site_type']).bool():\n",
    "        return (np.nan,np.nan)\n",
    "    elif isNaN(europe_meta[europe_meta['site']==code]['site_area']).bool():\n",
    "        return (np.nan,np.nan)   \n",
    "    else:\n",
    "        #Choose 2017-2021 data\n",
    "        importdata = pd.read_csv(str(code)+'.csv')\n",
    "        if (importdata.empty):\n",
    "            return (np.nan,np.nan)\n",
    "        else:\n",
    "            pollutant=importdata[\"variable\"].drop_duplicates()\n",
    "            if pol in pollutant.tolist():\n",
    "                importdata_pol = importdata[importdata[\"variable\"]==pol]\n",
    "                #check the unit to be ug.m-3\n",
    "                importdata_pol=importdata_pol[importdata_pol[\"unit\"]=='ug.m-3']\n",
    "                #remove zeros and negative values\n",
    "                importdata_pol=importdata_pol[importdata_pol[\"value\"]>0]\n",
    "                #Check if the amount of dataset is fewer than a year\n",
    "                if len(importdata_pol)<365*24:\n",
    "                    return (np.nan,np.nan)\n",
    "                else:                   \n",
    "                    len_validity = len(importdata_pol[importdata_pol[\"validity\"]>1])                  \n",
    "                    # Check if censored data consist 15% of the entire dataset\n",
    "                    if len_validity/len(importdata_pol)>0.15:\n",
    "                        return (np.nan,np.nan)                                                 \n",
    "                    else:\n",
    "                        data1 = importdata_pol[\"value\"]\n",
    "                        data2 = Counter(data1)\n",
    "                        # Check if the most occured data consist 15% of the entire dataset\n",
    "                        if data2.most_common(1)[0][1]/len(data1)>0.15:\n",
    "                            return (np.nan,np.nan)  \n",
    "                        else:                           \n",
    "                            data_full=np.array(data1)                           \n",
    "            else:\n",
    "                return (np.nan,np.nan)     \n",
    "        #plot histogram    \n",
    "        datarange=1.5*np.mean(data_full)\n",
    "        data_close_to_peak=data_full[data_full<datarange]\n",
    "        hist_lines=sns.distplot(data_close_to_peak, norm_hist=True, kde=True, color='gold',\n",
    "                                kde_kws={\"bw\":0.2,\"gridsize\":max(500,int(10*datarange))});\n",
    "        (xvalues_hist,yvalues_hist)=hist_lines.get_lines()[0].get_data()\n",
    "        #define the concentration which appears the most\n",
    "        peakOfdata=xvalues_hist[np.where(yvalues_hist==max(yvalues_hist))[0][0]]\n",
    "        #only consider the data higher than the peak \n",
    "        data_tail=data_full[data_full>=peakOfdata]\n",
    "        plt.clf()        \n",
    "        mslist=(np.mean(data_tail),np.std(data_tail))\n",
    "        return mslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Define a function which outputs q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_value_output(code,pol):\n",
    "    #check if the location has name\n",
    "    if isNaN(europe_meta[europe_meta['site']==code]['site_name']).bool():\n",
    "        return (np.nan,np.nan)\n",
    "    #check if the location can be found on map\n",
    "    elif math.isnan(europe_meta[europe_meta['site']==code]['latitude']):\n",
    "        return (np.nan,np.nan)\n",
    "    elif math.isnan(europe_meta[europe_meta['site']==code]['longitude']):\n",
    "        return (np.nan,np.nan)\n",
    "    #check if the area type is defined\n",
    "    elif isNaN(europe_meta[europe_meta['site']==code]['site_type']).bool():\n",
    "        return (np.nan,np.nan)\n",
    "    elif isNaN(europe_meta[europe_meta['site']==code]['site_area']).bool():\n",
    "        return (np.nan,np.nan)   \n",
    "    else:\n",
    "        #Choose 2017-2021 data\n",
    "        importdata = pd.read_csv(str(code)+'.csv')\n",
    "        if (importdata.empty):\n",
    "            return (np.nan,np.nan)\n",
    "        else:\n",
    "            pollutant=importdata[\"variable\"].drop_duplicates()\n",
    "            if pol in pollutant.tolist():\n",
    "                importdata_pol = importdata[importdata[\"variable\"]==pol]\n",
    "                #check the unit to be ug.m-3\n",
    "                importdata_pol=importdata_pol[importdata_pol[\"unit\"]=='ug.m-3']\n",
    "                #remove zeros and negative values\n",
    "                importdata_pol=importdata_pol[importdata_pol[\"value\"]>0]\n",
    "                #Check if the amount of dataset is fewer than a year\n",
    "                if len(importdata_pol)<365*24:\n",
    "                    return (np.nan,np.nan)\n",
    "                else:                   \n",
    "                    len_validity = len(importdata_pol[importdata_pol[\"validity\"]>1])                  \n",
    "                    # Check if censored data consist 15% of the entire dataset\n",
    "                    if len_validity/len(importdata_pol)>0.15:\n",
    "                        return (np.nan,np.nan)                                                 \n",
    "                    else:\n",
    "                        data1 = importdata_pol[\"value\"]\n",
    "                        data2 = Counter(data1)\n",
    "                        # Check if the most occured data consist 15% of the entire dataset\n",
    "                        if data2.most_common(1)[0][1]/len(data1)>0.15:\n",
    "                            return (np.nan,np.nan)  \n",
    "                        else:                           \n",
    "                            data_full=np.array(data1)                           \n",
    "            else:\n",
    "                return (np.nan,np.nan)                   \n",
    "                \n",
    "    #plot histograms\n",
    "    datarange=1.5*np.mean(data_full)\n",
    "    data_close_to_peak=data_full[data_full<datarange]\n",
    "    hist_lines=sns.distplot(data_close_to_peak, norm_hist=True, kde=True, color='gold',\n",
    "                            kde_kws={\"bw\":0.2,\"gridsize\":max(500,int(10*datarange))});\n",
    "    (xvalues_hist,yvalues_hist)=hist_lines.get_lines()[0].get_data()\n",
    "    #define the concentration which appears the most\n",
    "    peakOfdata=xvalues_hist[np.where(yvalues_hist==max(yvalues_hist))[0][0]]\n",
    "    #only consider the data higher than the peak\n",
    "    data_tail=data_full[data_full>peakOfdata]\n",
    "    plt.clf();\n",
    "\n",
    "    #generate an instance of the q-exponential, we need a=0 as lower bound for the support of the function            \n",
    "    q_exp=qExp_gen(name=\"q exponential\",a=0)    \n",
    "    #MLE for q-exponential distribution (built in function)    \n",
    "    parameters_Qexp_MLE=q_exp.fit(data_tail,1.1,1.1,floc=peakOfdata,fscale=1)\n",
    "    return parameters_Qexp_MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Define a function for classifying wind force scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ws_sort(code,pol):\n",
    "    #check if the location has name\n",
    "    if isNaN(europe_meta[europe_meta['site']==code]['site_name']).bool():\n",
    "        return (np.nan)\n",
    "    else:\n",
    "        #Enter the path of the folder of your svaed data\n",
    "        path = 'Enter the path of your saved data here'+str(code)+'.csv'\n",
    "        # path exists or not\n",
    "        if os.path.exists(path):\n",
    "            #Choose 2017-2021 data\n",
    "            importdata = pd.read_csv(path)\n",
    "            if (importdata.empty):\n",
    "                return (np.nan)\n",
    "            else:\n",
    "                pollutant=importdata[\"variable\"].drop_duplicates()\n",
    "                if pol in pollutant.tolist() and 'ws' in importdata.columns:\n",
    "                    #choose pollutant\n",
    "                    importdata_pol = importdata[importdata[\"variable\"]==pol]\n",
    "                    #only wind speed data corresponding to valid concentration data\n",
    "                    importdata_ws = importdata_pol.loc[importdata_pol['value']>0, 'ws']\n",
    "                    #remove negative values\n",
    "                    importdata_ws=importdata_ws[importdata_ws>=0]\n",
    "                    #remove NAN data\n",
    "                    #importdata_ws = [x for x in importdata_ws if str(x) != 'nan']\n",
    "                    #Check if the amount of dataset is fewer than a year\n",
    "                    if len(importdata_ws)<365*24:\n",
    "                        return (np.nan)\n",
    "                    else:\n",
    "                        data_full=np.array(importdata_ws)\n",
    "                        data_mean=np.mean(data_full)\n",
    "                        #print(data_mean)\n",
    "                        if data_mean<1.6:\n",
    "                            return \"Calm & Light Air\"\n",
    "                        elif data_mean>=1.6 and data_mean<3.4:\n",
    "                            return \"Light Breeze\"\n",
    "                        elif data_mean>=3.4 and data_mean<5.5:\n",
    "                            return \"Gentle Breeze\"\n",
    "                        elif data_mean>=5.5 and data_mean<8:\n",
    "                            return \"Moderate breeze\"\n",
    "                        elif data_mean>=8 and data_mean<10.8:\n",
    "                            return \"Fresh breeze\"\n",
    "                        elif data_mean>=10.8 and data_mean<13.9:\n",
    "                            return \"Strong breeze\"\n",
    "                        elif data_mean>=13.9 and data_mean<17.2:\n",
    "                            return \"Near Gale\"\n",
    "                        elif data_mean>=17.2 and data_mean<20.8:\n",
    "                            return \"Gale\"\n",
    "                        elif data_mean>=20.8 and data_mean<24.5:\n",
    "                            return \"Strong Gale\"                       \n",
    "                        elif data_mean>=24.5 and data_mean<28.5:\n",
    "                            return \"Storm\"\n",
    "                        elif data_mean>=28.5 and data_mean<32.7:\n",
    "                            return \"Violent storm\"\n",
    "                        elif data_mean>=32.7:\n",
    "                            return \"Hurricane\"                      \n",
    "                        else:\n",
    "                            return (np.nan) \n",
    "                        \n",
    "                else:\n",
    "                    return (np.nan)  \n",
    "        else:\n",
    "            return (np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6) Define a function which outputs log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the site code and pollutant type\n",
    "def log_likelihood_output(code,pol):\n",
    "    importdata = pd.read_csv(str(code)+'.csv')\n",
    "    importdata_pol = importdata[importdata[\"variable\"]==pol]\n",
    "    importdata_pol=importdata_pol[importdata_pol[\"unit\"]=='ug.m-3']\n",
    "    importdata_pol=importdata_pol[importdata_pol[\"value\"]>0]\n",
    "    data1 = importdata_pol[\"value\"]\n",
    "    data_full=np.array(data1)                           \n",
    "    \n",
    "    #plot histograms\n",
    "    datarange=1.5*np.mean(data_full)\n",
    "    data_close_to_peak=data_full[data_full<datarange]\n",
    "    hist_lines=sns.distplot(data_close_to_peak, norm_hist=True, kde=True, kde_kws={\"bw\":0.2,\"gridsize\":500});\n",
    "    (xvalues_hist,yvalues_hist)=hist_lines.get_lines()[0].get_data()\n",
    "    #define the concentration which appears the most\n",
    "    peakOfdata=xvalues_hist[np.where(yvalues_hist==max(yvalues_hist))[0][0]]\n",
    "    #only consider the data higher than the peak\n",
    "    data_tail=data_full[data_full>=peakOfdata]\n",
    "    plt.clf();\n",
    "\n",
    "    #generate an instance of the q-exponential, we need a=0 as lower bound for the support of the function            \n",
    "    q_exp=qExp_gen(name=\"q exponential\",a=0)  #a=peakofdata      \n",
    "    #MLE for q-exponential distribution (built in function)  #try different q,l   \n",
    "    parameters_Qexp_MLE=q_exp.fit(data_tail,1.1,1.1,floc=peakOfdata,fscale=1)  \n",
    "    #MLE for log-normal distribution \n",
    "    lognorm_param = stats.lognorm.fit(data_full, floc=0)\n",
    "    #weibull fit\n",
    "    weibull_param = stats.weibull_min.fit(data_full, floc=0)\n",
    "    #gamma fit\n",
    "    gamma_param=stats.gamma.fit(data_full,floc=0)\n",
    "    \n",
    "    #compute log-likelihood\n",
    "    q_likelihood = 0\n",
    "    l_likelihood = 0 \n",
    "    w_likelihood = 0\n",
    "    g_likelihood = 0    \n",
    "    \n",
    "    for item1 in data_tail:      \n",
    "        #choose q-value\n",
    "        q_likelihood = np.log(q_exp_func((item1-peakOfdata),parameters_Qexp_MLE[1],parameters_Qexp_MLE[0]))\n",
    "        q_likelihood += q_likelihood \n",
    "        #lognorm\n",
    "        l_likelihood = np.log(len(data_full)/len(data_tail)*lognorm.pdf(\n",
    "            item1,lognorm_param[0],lognorm_param[1],lognorm_param[2]))\n",
    "        l_likelihood += l_likelihood\n",
    "        #weibull\n",
    "        w_likelihood = np.log(len(data_full)/len(data_tail)*stats.weibull_min.pdf(\n",
    "            item1,weibull_param[0],loc = weibull_param[1], scale=weibull_param[2]))\n",
    "        w_likelihood += w_likelihood\n",
    "        #gamma\n",
    "        g_likelihood = np.log(len(data_full)/len(data_tail)*stats.gamma.pdf(\n",
    "            item1, gamma_param[0], loc=gamma_param[1], scale=gamma_param[2]))\n",
    "        g_likelihood += g_likelihood                 \n",
    "        \n",
    "    log_likelihood = (q_likelihood,l_likelihood,w_likelihood,g_likelihood)\n",
    "    return log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) set up dataframe for area type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the european locations\n",
    "europe_meta=pd.read_csv('data_sites.csv')\n",
    "df = pd.DataFrame({'site':europe_meta['site_name'],'code':europe_meta['site'],'type':europe_meta['site_type'],'area':europe_meta['site_area']})\n",
    "typelist=[]\n",
    "for x in europe_meta['site']:        \n",
    "    typelist.append(type_area_sort(x))    \n",
    "df[\"area_type\"] = typelist\n",
    "df.to_csv('European area type.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) set up dataframe for mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df = df\n",
    "mean_list=[]\n",
    "std_list=[]\n",
    "for x in europe_meta['site']:\n",
    "    mslist=statistics_output(x,'no')\n",
    "    mean_list.append(mslist[0])\n",
    "    std_list.append(mslist[1])   \n",
    "statistics_df[\"mean\"] = mean_list\n",
    "statistics_df[\"std\"] = std_list\n",
    "statistics_df.to_csv('europe no mean versus standard deviation.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df = df\n",
    "mean_list=[]\n",
    "std_list=[]\n",
    "for x in europe_meta['site']:\n",
    "    mslist=statistics_output(x,'no2')\n",
    "    mean_list.append(mslist[0])\n",
    "    std_list.append(mslist[1])   \n",
    "statistics_df[\"mean\"] = mean_list\n",
    "statistics_df[\"std\"] = std_list\n",
    "statistics_df.to_csv('europe no2 mean versus standard deviation.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df = df\n",
    "mean_list=[]\n",
    "std_list=[]\n",
    "for x in europe_meta['site']:\n",
    "    mslist=statistics_output(x,'pm2.5')\n",
    "    mean_list.append(mslist[0])\n",
    "    std_list.append(mslist[1])   \n",
    "statistics_df[\"mean\"] = mean_list\n",
    "statistics_df[\"std\"] = std_list\n",
    "statistics_df.to_csv('europe pm2.5 mean versus standard deviation.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_df = df\n",
    "mean_list=[]\n",
    "std_list=[]\n",
    "for x in europe_meta['site']:\n",
    "    mslist=statistics_output(x,'pm10')\n",
    "    mean_list.append(mslist[0])\n",
    "    std_list.append(mslist[1])   \n",
    "statistics_df[\"mean\"] = mean_list\n",
    "statistics_df[\"std\"] = std_list\n",
    "statistics_df.to_csv('europe pm10 mean versus standard deviation.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) set up dataframe for lambda and q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the european locations\n",
    "europe_meta=pd.read_csv('data_sites.csv')\n",
    "qlist=[]\n",
    "llist=[]\n",
    "qvalue_df = df\n",
    "for x in europe_meta['site']:        \n",
    "    parameters_Qexp_MLE = q_value_output(x,'no')\n",
    "    qlist.append(parameters_Qexp_MLE[1])\n",
    "    llist.append(parameters_Qexp_MLE[0])   \n",
    "qvalue_df[\"q\"] = qlist\n",
    "qvalue_df[\"l\"] = llist\n",
    "qvalue_df.to_csv('no q,lamda dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlist=[]\n",
    "llist=[]\n",
    "qvalue_df = df\n",
    "for x in europe_meta['site']:        \n",
    "    parameters_Qexp_MLE = q_value_output(x,'no2')\n",
    "    qlist.append(parameters_Qexp_MLE[1])\n",
    "    llist.append(parameters_Qexp_MLE[0])   \n",
    "qvalue_df[\"q\"] = qlist\n",
    "qvalue_df[\"l\"] = llist\n",
    "qvalue_df.to_csv('no2 q,lamda dataframe.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlist=[]\n",
    "llist=[]\n",
    "qvalue_df = df\n",
    "for x in europe_meta['site']:        \n",
    "    parameters_Qexp_MLE = q_value_output(x,'pm2.5')\n",
    "    qlist.append(parameters_Qexp_MLE[1])\n",
    "    llist.append(parameters_Qexp_MLE[0])   \n",
    "qvalue_df[\"q\"] = qlist\n",
    "qvalue_df[\"l\"] = llist\n",
    "qvalue_df.to_csv('pm2.5 q,lamda dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlist=[]\n",
    "llist=[]\n",
    "qvalue_df = df\n",
    "for x in europe_meta['site']:        \n",
    "    parameters_Qexp_MLE = q_value_output(x,'pm10')\n",
    "    qlist.append(parameters_Qexp_MLE[1])\n",
    "    llist.append(parameters_Qexp_MLE[0])   \n",
    "qvalue_df[\"q\"] = qlist\n",
    "qvalue_df[\"l\"] = llist\n",
    "qvalue_df.to_csv('pm10 q,lamda dataframe.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) set up dataframe for wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO\n",
    "ws_typelist=[]\n",
    "for x in europe_meta['site']:        \n",
    "    ws_typelist.append(ws_sort(x,'no'))    \n",
    "no_qvalue_df[\"ws_type\"] = ws_typelist\n",
    "no_qvalue_df.to_csv('no ws dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO2\n",
    "ws_typelist=[]\n",
    "for x in europe_meta['site']:        \n",
    "    ws_typelist.append(ws_sort(x,'no2'))    \n",
    "no2_qvalue_df[\"ws_type\"] = ws_typelist\n",
    "no2_qvalue_df.to_csv('no2 ws dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PM2.5\n",
    "ws_typelist=[]\n",
    "for x in europe_meta['site']:        \n",
    "    ws_typelist.append(ws_sort(x,'pm2.5'))    \n",
    "pm25_qvalue_df[\"ws_type\"] = ws_typelist\n",
    "pm25_qvalue_df.to_csv('pm2.5 ws dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PM10\n",
    "ws_typelist=[]\n",
    "for x in europe_meta['site']:        \n",
    "    ws_typelist.append(ws_sort(x,'pm10'))    \n",
    "pm10_qvalue_df[\"ws_type\"] = ws_typelist\n",
    "pm10_qvalue_df.to_csv('pm10 ws dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) set up dataframe for log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO\n",
    "q_likelihood_list=[]\n",
    "l_likelihood_list=[]\n",
    "w_likelihood_list=[]\n",
    "g_likelihood_list=[]\n",
    "\n",
    "likelihood_df = df\n",
    "for x in europe_meta['site']:        \n",
    "    log_likelihood = log_likelihood_output(x,'no')\n",
    "    q_likelihood_list.append(log_likelihood[0])\n",
    "    l_likelihood_list.append(log_likelihood[1])\n",
    "    w_likelihood_list.append(log_likelihood[2])\n",
    "    g_likelihood_list.append(log_likelihood[3])    \n",
    "likelihood_df[\"q_ll\"] = q_likelihood_list\n",
    "likelihood_df[\"l_ll\"] = l_likelihood_list\n",
    "likelihood_df[\"w_ll\"] = w_likelihood_list\n",
    "likelihood_df[\"g_ll\"] = g_likelihood_list\n",
    "\n",
    "likelihood_df.to_csv('no likelihood dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO2\n",
    "q_likelihood_list=[]\n",
    "l_likelihood_list=[]\n",
    "w_likelihood_list=[]\n",
    "g_likelihood_list=[]\n",
    "\n",
    "likelihood_df = df\n",
    "for x in europe_meta['site']:        \n",
    "    log_likelihood = log_likelihood_output(x,'no2')\n",
    "    q_likelihood_list.append(log_likelihood[1])\n",
    "    l_likelihood_list.append(log_likelihood[2])\n",
    "    w_likelihood_list.append(log_likelihood[3])\n",
    "    g_likelihood_list.append(log_likelihood[4])    \n",
    "likelihood_df[\"q_ll\"] = q_likelihood_list\n",
    "likelihood_df[\"l_ll\"] = l_likelihood_list\n",
    "likelihood_df[\"w_ll\"] = w_likelihood_list\n",
    "likelihood_df[\"g_ll\"] = g_likelihood_list\n",
    "\n",
    "likelihood_df.to_csv('no2 likelihood dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PM2.5\n",
    "q_likelihood_list=[]\n",
    "l_likelihood_list=[]\n",
    "w_likelihood_list=[]\n",
    "g_likelihood_list=[]\n",
    "\n",
    "likelihood_df = df\n",
    "for x in europe_meta['site']:        \n",
    "    log_likelihood = log_likelihood_output(x,'pm2.5')\n",
    "    q_likelihood_list.append(log_likelihood[0])\n",
    "    l_likelihood_list.append(log_likelihood[1])\n",
    "    w_likelihood_list.append(log_likelihood[2])\n",
    "    g_likelihood_list.append(log_likelihood[3])     \n",
    "likelihood_df[\"q_ll\"] = q_likelihood_list\n",
    "likelihood_df[\"l_ll\"] = l_likelihood_list\n",
    "likelihood_df[\"w_ll\"] = w_likelihood_list\n",
    "likelihood_df[\"g_ll\"] = g_likelihood_list\n",
    "\n",
    "likelihood_df.to_csv('pm2.5 likelihood dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PM10\n",
    "q_likelihood_list=[]\n",
    "l_likelihood_list=[]\n",
    "w_likelihood_list=[]\n",
    "g_likelihood_list=[]\n",
    "\n",
    "likelihood_df = df\n",
    "for x in europe_meta['site']:        \n",
    "    log_likelihood = log_likelihood_output(x,'pm10')\n",
    "    q_likelihood_list.append(log_likelihood[0])\n",
    "    l_likelihood_list.append(log_likelihood[1])\n",
    "    w_likelihood_list.append(log_likelihood[2])\n",
    "    g_likelihood_list.append(log_likelihood[3])    \n",
    "likelihood_df[\"q_ll\"] = q_likelihood_list\n",
    "likelihood_df[\"l_ll\"] = l_likelihood_list\n",
    "likelihood_df[\"w_ll\"] = w_likelihood_list\n",
    "likelihood_df[\"g_ll\"] = g_likelihood_list\n",
    "\n",
    "likelihood_df.to_csv('pm10 likelihood dataframe.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
